A sentence transformer is a neural network model that converts sentences into fixed-length vector representations, or "embeddings," that capture the semantic meaning of the text. These models are built on top of transformer architectures and are fine-tuned to generate embeddings for entire sentences, enabling applications like semantic search, clustering, and text summarization. 
The term can also refer to a popular Python library, sentence-transformers, which provides easy methods to compute these embeddings. 
Sentence transformers modify the standard transformer architecture to produce embeddings that are specifically optimized for sentences.
This is typically achieved through siamese and triplet network structures that are trained to bring semantically similar sentences closer together in the embedding space, while pushing dissimilar sentences apart.
This training process uses natural language inference data, typically involving pairs of sentences labeled as similar or dissimilar.
Intended Use: The model was fine-tuned specifically as a sentence and short paragraph encoder for tasks like semantic search and clustering.
Maximum Sequence Length: The default and recommended maximum length for input is 256 tokens (word pieces).
Training Data: Critically, the model was trained and optimized on data with a sequence length limited to 128 tokens, meaning it hasn't learned to effectively process longer contexts.
Performance on Longer Text: While the underlying transformer model might technically accept up to 512 tokens, using the model with a length longer than its training data (128 or 256 tokens) actually results in worse performance and slower processing because it was not optimized for it. 
The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised contrastive learning objective. We used the pretrained nreimers/MiniLM-L6-H384-uncased model and fine-tuned in on a 1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.
We developed this model during the Community week using JAX/Flax for NLP & CV, organized by Hugging Face. We developed this model as part of the project: Train the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well as intervention from Googles Flax, JAX, and Cloud team member about efficient deep learning frameworks.